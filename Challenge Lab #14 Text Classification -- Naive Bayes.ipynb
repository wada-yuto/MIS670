{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c407284",
   "metadata": {},
   "source": [
    "# Text Classification - Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed29083",
   "metadata": {},
   "source": [
    "*(For more information of Naive Bayes Classifier, please check the PowerPoint slides.)*\n",
    "\n",
    "We're working on **classification problem**. There are **different machine learning algorithms** available for building a predictive model\n",
    "\n",
    "<img src =\"http://amueller.github.io/sklearn_tutorial/cheat_sheet.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1615036e",
   "metadata": {},
   "source": [
    "The **fetch_20newsgroups()** function allows the loading of filenames and data from the 20 newsgroups dataset. It has 20 classes, 18846 observations, and features in the form of strings. It downloads the dataset from the original 20 newsgroups website and caches it locally.\n",
    "\n",
    "The 20 newsgroups dataset splits in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n",
    "\n",
    "https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "520be45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "#Load the filenames and data from the 20 newsgroups dataset (classification).\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec93695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit: http://qwone.com/~jason/20Newsgroups/\n",
    "data = fetch_20newsgroups()\n",
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedbef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 20 classes data\n",
    "categories = ['alt.atheism', 'comp.graphics', \n",
    "              'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', \n",
    "              'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', \n",
    "              'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', \n",
    "              'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', \n",
    "              'sci.space', 'soc.religion.christian', 'talk.politics.guns', \n",
    "              'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74b6228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first news article in train \n",
    "print(train.data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first news article in test\n",
    "print(test.data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d3bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many articles in train and test\n",
    "print(len(train.data)) #60% of the total data\n",
    "print(len(test.data)) #40% of the total data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c695d08",
   "metadata": {},
   "source": [
    "## TF-IDF \n",
    "\n",
    "short for **Term Frequency–inverse Document Frequency** is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus -- a language resource consisting of a large and structured set of texts (nowadays usually electronically stored and processed).\n",
    "\n",
    "- TF: measures how frequently a term appears \n",
    "    - = Number of times the word appears in a document / Total number of words in the document\n",
    "<br><br>    \n",
    "- IDF: measures the relative importance of a word. for example, such words as \"at\" and \"of\" frequently appear, but little important. IDF **weight down such frequent terms while scale up the rare words** \n",
    "    - = log(total number of documents / number of documents containing the word in question)\n",
    "<br><br> \n",
    "- TFIDF: the importance of words or tokens (or features) in a document\n",
    "    - = tf * idf\n",
    "    - tells the importance of words, which is used in classification\n",
    "\n",
    "It is often used as a **weighting factor** in searches of \n",
    "- information retrieval\n",
    "- text mining\n",
    "- user modeling\n",
    "\n",
    "The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today.\n",
    "\n",
    "sources: \n",
    "- https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "- https://en.wikipedia.org/wiki/Text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90887a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library for TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the top words based on the TFIDF scores\n",
    "tfIdfVectorizer=TfidfVectorizer(use_idf=True,stop_words='english')\n",
    "\n",
    "tfIdf = tfIdfVectorizer.fit_transform(train.data)\n",
    "\n",
    "df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "df = df.sort_values('TF-IDF', ascending=False)\n",
    "print (df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575dc6f5",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier \n",
    "\n",
    "**The Multinomial Naive Bayes** calculates each lebal's likelihood for a given sample and outputs the tag with the greatest chance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3db5447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd2d285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model based on Multinomial Naive Bayes using make_pipeline\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81f7d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model with the train data\n",
    "model.fit(train.data, train.target) #The target attribute is the integer index of the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37529e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating labels for the test data\n",
    "labels = model.predict(test.data)\n",
    "print(labels[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bc867d",
   "metadata": {},
   "source": [
    "### How did we got the labels for testing data?\n",
    "\n",
    "<img src=\"http://www.nltk.org/images/supervised-classification.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecde2ad",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in an actual class while each column represents the instances in a predicted class, or vice versa – both variants are found in the literature. The name stems from the fact that it makes it easy to see whether the system is confusing two classes (i.e. commonly mislabeling one as another).\n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab6035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb05ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(test.target, labels)\n",
    "# T means Transpose\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=train.target_names, yticklabels=train.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726dbb81",
   "metadata": {},
   "source": [
    "## Calculate the accuracy of the Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689d599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(test.target, labels)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7eec38",
   "metadata": {},
   "source": [
    "## Write a fuction for future use to predict news category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ce2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting category on new data based on trained model\n",
    "def predict_category(s, train=train, model=model): #s: set as string, model=model:make_pipeline\n",
    "    pred = model.predict([s]) #set the data to pipeline: Tokenized, ... NB\n",
    "    return train.target_names[pred[0]] #train.target_names = categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7797ba40",
   "metadata": {},
   "source": [
    "## Let's test our trained model with a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064f4c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"articles1.csv\",header=0)\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8bb893",
   "metadata": {},
   "outputs": [],
   "source": [
    "news.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f144c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into smaller dataset\n",
    "news_test = news.iloc[:10]\n",
    "news_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e98f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using our NB model to classify the news\n",
    "\n",
    "cate = []\n",
    "\n",
    "for news in news_test[\"content\"]:\n",
    "    category = predict_category(news)\n",
    "    \n",
    "    cate.append(category)\n",
    "\n",
    "catedf = pd.DataFrame(cate, columns=[\"Category\"])\n",
    "catedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d26b8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two dataframe\n",
    "news_test = pd.concat([news_test, catedf], axis=1, join=\"inner\")\n",
    "news_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c18797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_colwidth\", 500)\n",
    "\n",
    "print(news_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f10b4",
   "metadata": {},
   "source": [
    "# Actions: Create a Spam filter for the text messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1522e75",
   "metadata": {},
   "source": [
    "### Instructions:\n",
    "\n",
    "1. Clean the sms texts\n",
    "2. Conduct the feature engineering (Words to Vectors)\n",
    "    - Tokenization\n",
    "    - Word Frequency\n",
    "    - Stemming\n",
    "    - Lemmatization\n",
    "    - Remove stopwords\n",
    "3. Calculate the TF-IDF\n",
    "4. Split the data into training and testing dataset\n",
    "5. Build a Spam filter using Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e575723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51652174",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMS = pd.read_csv('SpamSMStraining.txt', sep = '\\t', header=None, names=[\"label\", \"sms\"])\n",
    "SMS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae5b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the stopwords from sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6150dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming the sms texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5413cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cbdf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Word Frequency (counting the frequency of each word appears)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39872f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393262fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Creating training and test sets (80-20): X = corpus; y = classifications\n",
    "x_train, x_test, y_train, y_test = train_test_split(SMS[\"sms\"], SMS[\"label\"], test_size=0.2, random_state=10)\n",
    "len(x_train), len(y_train), len(x_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bbe7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test[:5])\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5abd7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model based on Multinomial Naive Bayes using make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd3eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model with the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59e0c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating labels for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb903c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the accuracy of your Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284f7163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a fuction for future use to predict the spam messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233f992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your function with new five sms messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6745beb",
   "metadata": {},
   "source": [
    "# This is what happens when you reply to spam email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be943ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo('4o5hSxvN_-s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd74a80f",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "- https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html\n",
    "- https://www.youtube.com/watch?v=l3dZ6ZNFjo0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5ca7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
